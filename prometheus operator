下载地址：[https://github.com/prometheus-operator/kube-prometheus](https://github.com/prometheus-operator/kube-prometheus)
# 安装Pormetheus
## Step1 下载prometheus-operator
```shell
$ wget https://github.com/prometheus-operator/kube-prometheus/archive/refs/tags/v0.9.0.tar.gz
$ tar -zxvf kube-prometheus-0.9.0.tar.gz
$ cd kube-prometheus-0.9.0
manifests 文件夹下有prometheus、grafana、alertmanager等文件

# 先创建namespace
$ kubectl apply -f manifests/setup
# 再安装组件
$ kubectl apply -f manifests/

# 检查pod启动是否正常
$ kubectl get pods -n monitoring
NAME                                   READY   STATUS    RESTARTS   AGE
alertmanager-main-0                    2/2     Running   0          6h45m
alertmanager-main-1                    2/2     Running   0          6h45m
alertmanager-main-2                    2/2     Running   0          6h45m
blackbox-exporter-6798fb5bb4-wplvg     3/3     Running   0          6h45m
grafana-7476b4c65b-mjxrr               1/1     Running   0          6h45m
kube-state-metrics-8556db9dc4-6tn4k    3/3     Running   0          6h18m
node-exporter-fvvdd                    2/2     Running   0          6h45m
node-exporter-hwl8d                    2/2     Running   0          6h45m
node-exporter-jpl5z                    2/2     Running   0          6h45m
node-exporter-k8w5w                    2/2     Running   0          6h45m
node-exporter-kts2d                    2/2     Running   0          6h45m
node-exporter-p99jq                    2/2     Running   0          6h45m
node-exporter-rz7k4                    2/2     Running   0          6h45m
prometheus-adapter-7bb4bc5d76-jmbqx    1/1     Running   0          6h21m
prometheus-adapter-7bb4bc5d76-rnpt5    1/1     Running   0          6h21m
prometheus-k8s-0                       2/2     Running   0          6h45m
prometheus-k8s-1                       2/2     Running   0          6h45m
prometheus-operator-75d9b475d9-7hjsd   2/2     Running   0          9h
$ kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.103.147.194   <none>        9093/TCP                     6h46m
alertmanager-operated   ClusterIP   None             <none>        9093/TCP,9094/TCP,9094/UDP   6h46m
blackbox-exporter       ClusterIP   10.111.7.171     <none>        9115/TCP,19115/TCP           6h46m
grafana                 ClusterIP   10.97.251.223    <none>        3000/TCP                     6h46m
kube-state-metrics      ClusterIP   None             <none>        8443/TCP,9443/TCP            6h46m
node-exporter           ClusterIP   None             <none>        9100/TCP                     6h46m
prometheus-adapter      ClusterIP   10.103.163.3     <none>        443/TCP                      6h46m
prometheus-k8s          ClusterIP   10.104.251.163   <none>        9090/TCP                     6h46m
prometheus-operated     ClusterIP   None             <none>        9090/TCP                     6h46m
prometheus-operator     ClusterIP   None             <none>        8443/TCP                     9h


问题：pod启动的时候有些会启动不成功，describe 主要原因是因为镜像源是k8s.gcr.io
解决办法：
k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.9.1改为registry.cn-beijing.aliyuncs.com/zhaohongye/prometheus-adapter:v0.9.1
k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.3.0改为registry.cn-beijing.aliyuncs.com/zhaohongye/kube-state-metrics:v1.9.5

```
## Step2 traefik路由grafana、prometheus
```shell
# 创建ingressroute
$ vim prometheus-ingressroute.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: prometheus
  namespace: monitoring
spec:
  entryPoints:
    - websecure
  routes:
  - match: Host(`domain`)
    kind: Rule
    services:
    - name: prometheus-k8s # 指向prometheus service
      port: 9090
  tls:
    secretName: prometheus-k8s
$ vim grafana-ingressroute.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: grafana
  namespace: monitoring
spec:
  entryPoints:
    - websecure
  routes:
  - match: Host(`domain`)
    kind: Rule
    services:
    - name: grafana # 指向grafana service
      port: 3000
  tls:
    secretName: grafana-datasources
$ vim alertmanager-ingressroute.yaml
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  entryPoints:
    - websecure
  routes:
  - match: Host(`domain`)
    kind: Rule
    services:
    - name: alertmanager-main # 指向alertmanager service
      port: 9093
  tls:
    secretName: prometheus-main

# 本地配置hosts
$ vim /etc/hosts

```
# 配置ServiceMonitor
## 监控kube-scheduler
```shell
$ vim prometheus-serviceMonitorKubeScheduler.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: kube-scheduler
  name: kube-scheduler
  namespace: monitoring
spec:
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token  # token 文件
    interval: 30s  # 每30s获取一次信息
    port: https-metrics  # 对应 service 的端口名
    scheme: https  # 注意是使用 https 协议
    tlsConfig:  # 跳过安全校验
      insecureSkipVerify: true
  jobLabel: k8s-app  # 用于从中检索任务名称的标签
  namespaceSelector:  # 表示去匹配某一命名空间中的 Service，如果想从所有的namespace中匹配用any:true
    matchNames:
    - kube-system
  selector:  # 匹配的 Service 的 labels，如果使用 mathLabels，则下面的所有标签都匹配时才会匹配该 service，如果使用 matchExpressions，则至少匹配一个标签的 service 都会被选择
    matchLabels:
      k8s-app: kube-scheduler
$ vim prometheus-kubeSchedulerService.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-scheduler
  labels:  # 必须和上面的 ServiceMonitor 下面的 matchLabels 保持一致
    k8s-app: kube-scheduler
spec:
  selector:
    component: kube-scheduler
  ports:
  - name: https-metrics
    port: 10259
    targetPort: 10259  # 需要注意现在版本默认的安全端口是10259

创建完成后，隔一小会儿后去 Prometheus 页面上查看 targets 下面 kube-scheduler 已经有采集的目标了，但是报了 connect: connection refused 这样的错误

解决：因为 kube-scheduler 启动的时候默认绑定的是 127.0.0.1 地址，所以要通过 IP 地址去访问就被拒绝了
$ vim /etc/kubernetes/manifests/kube-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=0.0.0.0  # 修改为0.0.0.0
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true

```
## 监控kube-controller-manager
```shell
$ cat prometheus-kubeControllerManagerService.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-controller-manager
  labels:
    k8s-app: kube-controller-manager
spec:
  selector:
    component: kube-controller-manager
  ports:
  - name: https-metrics
    port: 10257
    targetPort: 10257  # controller-manager 的安全端口为10257
$ cat prometheus-serviceMonitorKubeControllManagerService.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: kube-controller-manager
  name: kube-controller-manager
  namespace: monitoring
spec:
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token  # token 文件
    interval: 30s  # 每30s获取一次信息
    port: https-metrics  # 对应 service 的端口名
    scheme: https  # 注意是使用 https 协议
    tlsConfig:  # 跳过安全校验
      insecureSkipVerify: true
  jobLabel: k8s-app  # 用于从中检索任务名称的标签
  namespaceSelector:  # 表示去匹配某一命名空间中的 Service，如果想从所有的namespace中匹配用any:true
    matchNames:
    - kube-system
  selector:  # 匹配的 Service 的 labels，如果使用 mathLabels，则下面的所有标签都匹配时才会匹配该 service，如果使用 matchExpressions，则至少匹配一个标签的 service 都会被选择
    matchLabels:
      k8s-app: kube-controller-manager
```
## 监控kube-proxy
```shell
# 修改kube-proxy配置文件
$ kubectl edit configmap kube-proxy -n kube-system
   minSyncPeriod: 0s
      scheduler: ""
      syncPeriod: 30s
    kind: KubeProxyConfiguration
    metricsBindAddress: thanks 0.0.0.0:10249  # 修改此处
    mode: "ipvs"                         
    nodePortAddresses: null
# 创建kube-proxy service
$ cat prometheus-kubeproxyService.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-proxy
  labels:
    k8s-app: kube-proxy
spec:
  selector:
    k8s-app: kube-proxy
  ports:
  - name: https-metrics
    port: 10249
    targetPort: 10249  # controller-manager 的安全端口为10249
# 创建 kube-proxy ServiceMonitor
$ cat prometheus-serviceMonitorKubeProxy.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: kube-proxy
  name: kube-proxy
  namespace: monitoring
spec:
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token  # token 文件
    interval: 30s  # 每30s获取一次信息
    port: https-metrics  # 对应 service 的端口名
    scheme: https  # 注意是使用 https 协议
    tlsConfig:  # 跳过安全校验
      insecureSkipVerify: true
  jobLabel: k8s-app  # 用于从中检索任务名称的标签
  namespaceSelector:  # 表示去匹配某一命名空间中的 Service，如果想从所有的namespace中匹配用any:true
    matchNames:
    - kube-system
  selector:  # 匹配的 Service 的 labels，如果使用 mathLabels，则下面的所有标签都匹配时才会匹配该 service，如果使用 matchExpressions，则至少匹配一个标签的 service 都会被选择
    matchLabels:
      k8s-app: kube-proxy
```
# 配置自定义监控项

- 第一步建立一个 ServiceMonitor 对象，用于 Prometheus 添加监控项
- 第二步为 ServiceMonitor 对象关联 metrics 数据接口的一个 Service 对象
- 第三步确保 Service 对象可以正确获取到 metrics 数据
```shell
explain: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Alertmanager

$ kubectl get pods etcd-master1 -n kube-system -o yaml
....
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.168.31.75:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --initial-advertise-peer-urls=https://192.168.31.75:2380
    - --initial-cluster=master1=https://192.168.31.75:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.168.31.75:2379
    - --listen-metrics-urls=http://0.0.0.0:2381  #指标监听地址,修改成0.0.0.0
    - --listen-peer-urls=https://192.168.31.75:2380
....
# 1.创建Servicemonitor对象
$ cat prometheus-serviceMonitorEtcd.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: etcd-k8s
  namespace: monitoring
  labels:
    k8s-app: etcd-k8s
spec:
  jobLabel: k8s-app
  endpoints:
    - port: port
      interval: 15s
  selector:
    matchLabels:
      k8s-app: etcd
  namespaceSelector:
    matchNames:
      - kube-system

# 2.创建etcd service 与servicemonitor关联
$ cat etcd-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
spec:
  type: ClusterIP
  clusterIP: None # 一定要设置 clusterIP:None
  ports:
    - name: port
      port: 2381
---
apiVersion: v1
kind: Endpoints
metadata:
  name: etcd-k8s
  namespace: kube-system
  labels:
    k8s-app: etcd
subsets:
  - addresses:
      - ip: 192.168.31.75 # 指定etcd节点地址，如果是集群则继续向下添加
        nodeName: etc-master
    ports:
      - name: port
        port: 2381
# 3. 配置监控项
数据采集到后，可以在 grafana 中导入编号为 3070 的 dashboard，就可以获取到 etcd 的监控图表
```
# 配置 PrometheusRule
Prometheus operator中通过role 为 endpoints 的 kubernetes 的自动发现机制获取的，匹配的是服务名为 alertmanager-main，端口名为 web 的 Service 服务
告警配置文档：[https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/alerting.md](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/alerting.md)
```shell
alerting:
  alert_relabel_configs:
    - separator: ;
      regex: prometheus_replica
      replacement: $1
      action: labeldrop
  alertmanagers:
    - kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
              - monitoring
      scheme: http
      path_prefix: /
      timeout: 10s
      api_version: v1
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_name]
          separator: ;
          regex: alertmanager-main
          replacement: $1
          action: keep
        - source_labels: [__meta_kubernetes_endpoint_port_name]
          separator: ;
          regex: web
          replacement: $1
          action: keep
rule_files:
  - /etc/prometheus/rules/prometheus-k8s-rulefiles-0/*.yaml

# 查看alertmanager-main 服务
$ kubectl describe svc alertmanager-main -n monitoring

# /etc/prometheus/rules/prometheus-k8s-rulefiles-0/ 目录查看yaml文件
$ kubectl exec -it prometheus-k8s-0 /bin/sh -n monitoring
$ /prometheus $ ls /etc/prometheus/rules/prometheus-k8s-rulefiles-0/
YAML 文件实际上就是我们之前创建的一个 PrometheusRule文件
$ vim alertmanager-prometheusRule.yaml
...
spec:
  groups:
  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerFailedReload
      annotations:
        description: Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.
        runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload
        summary: Reloading an Alertmanager configuration has failed.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="monitoring"}[5m]) == 0
      for: 10m
      labels:
        severity: critical
...

# rule规则过滤器
$ cat prometheus-rules.yaml
这里有个很重要的属性ruleSelector,用于匹配rule规则的过滤器
ruleSelector:
  matchLabels:
    prometheus: k8s
    role: alert-rules
```
如果我们需要自定义一个报警选项的话，只需要定义一个 PrometheusRule 资源对象即可
```shell
注意：
要想自定义一个报警规则，只需要创建一个具有 prometheus=k8s 和 role=alert-rules 标签的 PrometheusRule 对象就可以了。

# 查看告警规则
$ kubectl get PrometheusRule -n monitoring
NAME                              AGE
alertmanager-main-rules           7h55m
kube-prometheus-rules             7h55m
kube-state-metrics-rules          7h55m
kubernetes-monitoring-rules       7h55m
node-exporter-rules               7h55m
prometheus-k8-prometheus-rules   7h55m
prometheus-operator-rules         7h55m
$ kubectl get prometheusrule alertmanager-main-rules -n monitoring -o yaml

# 创建prometheus-roule
$ cat prometheus-etcdRules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels: # 一定要有prometheus: k8s、role: alert-rules标签
    prometheus: k8s
    role: alert-rules
  name: etcd-rules
  namespace: monitoring
spec:
  groups:
    - name: etcd
      rules:
        - alert: EtcdClusterUnavailable
          annotations:
            summary: etcd cluster small
            description: If one more etcd peer goes down the cluster will be unavailable
          expr: |
            count(up{job="etcd"} == 0) > (count(up{job="etcd"}) / 2 - 1)
          for: 3m
          labels:
            severity: critical

```
# 自动发现配置
我们可以在 Prometheus Operator 当中去自动发现并监控具有prometheus.io/scrape=true 这个 annotations 的 Service
```shell
要想自动发现集群中的 Service，就需要我们在 Service 的 annotation 区域添加 prometheus.io/scrape=true 的声明

$ vim prometheus-additional.yaml
- job_name: "kubernetes-endpoints"
  kubernetes_sd_configs:
    - role: endpoints
  relabel_configs:
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
      action: replace
      target_label: __scheme__
      regex: (https?)
    - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels:
        [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
      action: replace
      target_label: __address__
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2
    - action: labelmap
      regex: __meta_kubernetes_service_label_(.+)
    - source_labels: [__meta_kubernetes_namespace]
      action: replace
      target_label: kubernetes_namespace
    - source_labels: [__meta_kubernetes_service_name]
      action: replace
      target_label: kubernetes_name
    - source_labels: [__meta_kubernetes_pod_name]
      action: replace
      target_label: kubernetes_pod_name

# 创建对应的secret对象
$ kubectl create secret generic additional-configs --from-file=prometheus-additional.yaml -n monitoring
secret "additional-configs" created

# prometheus添加属性
$ vim prometheus-prometheus.yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  labels:
    prometheus: k8s
  name: k8s
  namespace: monitoring
spec:
  alerting:
    alertmanagers:
      - name: alertmanager-main
        namespace: monitoring
        port: web
  image: prom/prometheus:v2.26.0 # 使用最新版本的镜像
  nodeSelector:
    kubernetes.io/os: linux
  podMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  probeNamespaceSelector: {}
  probeSelector: {}
  replicas: 2
  resources:
    requests:
      memory: 400Mi
  ruleSelector: # 用来匹配rule规则的selector
    matchLabels: # 匹配的是具有下面两个标签的PrometheusRule这个资源对象
      prometheus: k8s
      role: alert-rules
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  version: v2.26.0
  additionalScrapeConfigs: # 添加additionalScrapeConfigs属性
    name: additional-configs
    key: prometheus-additional.yaml

# 给prometheus-k8s 增加权限
$ vim prometheus-clusterRole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
  - apiGroups:
      - ""
    resources:  # 增加services、pods资源
      - nodes
      - services
      - endpoints
      - pods
      - nodes/proxy
    verbs:  # 增加list权限
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps
      - nodes/metrics
    verbs:
      - get
  - nonResourceURLs:
      - /metrics
    verbs:
      - get

# 更新
$ kubectl apply -f prometheus-prometheus.yaml

```
# 数据持久化
这里通过 storageclass 来做数据持久化
```shell
# 通过API查看prometheus如何配置存储数据
$ kubectl explain prometheus.spec.storage

# 我们通过storage属性配置volumeClaimTemplate对象即可
$ cat prometheus-prometheus.yaml
......
storage:
  volumeClaimTemplate:
    spec:
      storageClassName: rook-ceph-block
      resources:
        requests:
          storage: 20Gi
然后更新 prometheus 这个 CRD 资源，更新完成后会自动生成两个 PVC 和 PV 资源对象
$ kubectl apply -f prometheus-prometheus.yaml
prometheus.monitoring.coreos.com/k8s configured
$ kubectl get pvc -n monitoring
NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
prometheus-k8s-db-prometheus-k8s-0   Bound    pvc-79ad4856-2ab0-4445-814f-958a4699fab9   20Gi       RWO            rook-ceph-block   56s
prometheus-k8s-db-prometheus-k8s-1   Bound    pvc-8eae438e-bf7f-41a3-ae58-d7018c727866   20Gi       RWO            rook-ceph-block   55s
$ kubectl get pv |grep monitoring
pvc-79ad4856-2ab0-4445-814f-958a4699fab9   20Gi       RWO            Retain           Bound      monitoring/prometheus-k8s-db-prometheus-k8s-0          rook-ceph-block            90s
pvc-8eae438e-bf7f-41a3-ae58-d7018c727866   20Gi       RWO            Retain           Bound      monitoring/prometheus-k8s-db-prometheus-k8s-1          rook-ceph-block            90s


```
